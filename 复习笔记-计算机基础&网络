位运算

	真值与机器数
		​“真值”指的就是数本身，例如-10，真值就是-10
		​一个数在计算机中的二进制表示形式，叫做这个数的机器数
		​在计算机中，用来表示有符号数的机器数有三种，即原码、反码、补码
		​三种表示方法均有“符号位”和“数值位”两部分，符号位都是占据最高位，用0表示“正数”，用1表示“负数”

		问：8位二进制数可以表示的数值范围是多少，99%的人张口就来：-128~127
		ok，8位二进制数，最高位需要用来表示符号，那么剩下的7位用来表示数值
		于是，最大数为1111 1111=>+127，最小0111 1111=>-127，卧槽，得出的结论是8位二进制数可以表示的数值范围是-127到+127
		灵魂拷问：-128到底怎么来的？？？
		​真相是这样的：
		8位二进制数用7位表示数值，那么7位2进制数000 0000的值为0
		那么,它前面加上符号位0,还表示0吧?
		那好,如果它前面加上1呢,仍然表示0?这不是重复了么?
		一个0,怎么用两个值来表示呢！所以1000 0000就表示-128啦

	原码、反码、补码
		1、正数：原码、反码、补码都一样
		   真值：3
		   原码：0000 0011   最高位为0，表示正数
		   反码：0000 0011
		   补码：0000 0011

		2、负数：原码、反码、补码不同
		   真值：-3
		   原码：1000 0011   最高位为1，表示负数
		   反码：1111 1100   由原码演变而来，原码的符号位不变，数值位全部取反
		   补码：1111 1101   在反码的基础上+1
		在计算机系统中，数值一律用补码来存储！主要原因：使用补码，可以将符号位和其它位统一处理；同时，减法也可按加法来处理，需要注意的是两个用补码表示的数相加时，如果最高位（符号位）有进位，则进位被舍弃，处理完后我们如上图所示用补码反推出真值即可，例如计算机在计算8-3的时候，会这么做8+（-3），具体如下:
		第一步：真值->原码->反码->补码
		    真值：8
		    原码：0000 1000  
		    反码：0000 1000
		    补码：0000 1000
	    	真值：-3
		    原码：1000 0011  
		    反码：1111 1100
		    补码：1111 1101
		第二步：补码之间的运算，此处为相加
		    8的补码：0000 1000
		    -3的补码：1111 1101
		    相加得补码：0000 0101 # 补码相加，高位有进位会被舍弃
		第三步：补码->反码->原码->真值
		    上一步得到的补码结果：0000 0101
		    符号位是0，为正数，那么就简单了，正数的原、反、补码都一样，所以一步到位
		    补码->反码->原码：0000 0101
		    原码->真值：5
		有了补码以后，减法都可以当做加法去运算，你可知道，这将极大地简化计算机的运算设计。不仅如此，我们即将介绍的位运算也都是基于补码进行的

	位运算
		按位与&：两位全为1，结果才为1，否则为0
		按位或|：两位只要存在一个1，结果就为1，否则为0
		按位异或^：只有在两位不相同，即一个为0一个为1的情况下，结果才为1，否则为0
		<< n：各二进制位全部左移n位，高位丢弃，低位补0
		>> n: 各二进制位全部右移n位，如果是正数，则高位补0，如果是负数则高位补1

	位运算高级操作
		位运算是cpu直接支持的，效率最高，位运算可能在平常的编程中使用的并不多，但涉及到底层优化，一些算法及源码可能会经常遇见，下面来介绍一下风骚的操作
		
		用位运算 & 取代 % 取模
		X % 2^n = X & (2^n – 1) 注意：用位运算 & 来取代 % 取模需要被取模的数必须是2的幂才成立

		将一个数左移 n 位，相当于乘以了 2 的 n 次方，右移n位，相当于除以2的n次方取整

		判断奇偶
		我们可以利用 & 运算符的特性，来判断二进制数第一位是0还是1。用if ((a & 1) == 0) 代替 if (a % 2 == 0)来判断a是不是偶数。

		交互数值
		a:=10
	    b:=20
	    a ^= b;
	    b ^= a;
	    a ^= b;

	    类似于bitmap存储并计算，详见https://zhuanlan.zhihu.com/p/110407440


CPU
	cpu组成：运算单元（逻辑门）+控制单元（包含一些指令寄存器）+存储单元（各种类型的寄存器+三级高速缓存）+信号时钟

	寄存器
		cpu中的寄存器是用于存储和处理数据及控制指令等的特殊存储区域，具有以下作用：
		存储数据：寄存器是CPU内存储器中存储数据最快的一种方式，可以快速的存储、读取和处理数据
		存储指令：指令寄存器存储下一条要执行的指令，CPU从指令寄存器中读取指令并解码执行
		存储地址：地址寄存器用于存储内存地址，方便CPU访问内存中的数据
		CPU执行的每条指令都由操作码和操作数组成，简单理解就是要对谁（操作数）做什么（操作码）。在CPU内部要运算的数据总是放在寄存器中，而实际的数据则有可能是放在内存或者是IO端口中。因为我们的程序其实大部分时间就是做了如下三件事：
		1.把内存或者IO端口的数据读取到寄存器
		2.将寄存器中的数据进行运算
		3.将寄存器的内容写到内存或者IO端口中
		这三件事都是与寄存器有关，寄存器就是数据存储的中转站，非常关键，在CPU所提供的指令中，如果操作数有两个时至少要有一个是寄存器

		多线程下的寄存器复用
		既然有多线程存在，而CPU中的寄存器又只有那么一套，如果不加处理岂不会产生数据错乱的场景？答案是否定的，我们知道线程是一个进程中的执行单元。每个线程的调度执行其实都是通过操作系统来完成，也就是说哪个线程占有CPU执行以及执行多久都是由操作系统控制的。具体的实现是每创建一个线程时都会为这个线程创建一个数据结构来保存这个线程的信息，我们称这个数据结构为线程上下文，每个线程的上下文中有一部分数据是用来保存当前所有寄存器的副本。每当操作系统暂停一个线程时，就会将CPU中的所有寄存器的当前内容都保存到线程上下文数据结构中。而操作系统要让另外一个线程执行时将要执行的线程的上下文中保存的所有寄存器的内容再写回到CPU中，并将要运行的线程中上次保存暂停的指令也赋值给CPU的指令寄存器，并让新线程再次执行。CPU进行进程或者线程切换时，CPU寄存器内的当前内容会存到对应堆栈中，再将下一个要执行的进程或线程堆栈中内容加载进寄存器。

	高速缓存
		运算的问题通过寄存器解决了，还存在一个问题：我们知道程序在运行时是要将所有可执行的二进制指令代码都装载到内存里面去，CPU每执行一条指令前都需要从内存中将指令读取到CPU内并执行。如果按这样每次都从内存读取一条指令来依次执行的话，还是存在着CPU和内存之间的处理瓶颈问题，从而造成整体性能的下降。这个问题该怎么解决？答案就是高速缓存，其实在CPU内部不仅有为解决运算问题而设计的寄存器，还集成了一个部分高速缓存区域。高速缓存的制造成本要比寄存器低，但是比内存的制造成本高，容量要比寄存器大，但是比内存的容量小很多。虽然没有寄存器和运算器之间的距离那么紧密，但是要比内存到运算器之间的距离近很多。一般来说CPU内的高速缓存可能只有几KB或者几十KB，正是通过高速缓存的引入，当程序在运行时，就可以预先将部分在内存中要执行的指令代码以及数据复制到高速缓存中去，而CPU则不再每次都从内存中读取指令而是直接从高速缓存中读取指令来执行，从而加快了整体的速度。当然要预读取哪块内存区域的指令和数据到缓存上以及怎么去读取这些工作都交给操作系统去调度完成。
		在实际中CPU内部可能不止有一级高速缓存，有可能会配备两到三级高速缓存，比如iphonex上的arm处理器里除了有固定的37个通用寄存器外，L1级缓存的容量是64KB，L2级达到了8MB。

内存
	RAM：全称random access memory 即内存，特点是断电后数据消失，DRAM动态随机存取存储器：用作主存储器即内存条，SRAM静态随机存取存储器：用作高速缓存
	ROM：只读存储器readonly memory，主要是用来存储一些系统信息，或者启动程序BIOS程序
	Flash闪存：SSD、U盘等

	虚拟内存
	CPU并不会直接跟内存物理地址交互，而是通过一个叫内存管理单元（MMU），来将虚拟地址转换为实际的物理地址，有了MMU之后，就不会出现数据相互影响的情况了。操作系统会给每一个进程分配独立的一套虚拟地址，各个进程之间互不干涉。通过虚拟内存来管理实际的物理地址，每一个进程申请的物理地址，因为有虚拟内存的统一管理，所以不会出现互相影响的情况。
	虚拟内存空间的内部又被分为内核空间和用户空间两部分，32位系统内核空间占用1G，剩下3G是用户空间，而64位系统内核空间和用户空间都是128T，进程在用户态时，只能访问用户空间内存，只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。
	内存是以页为单位进行管理，一页通常为4KB，例如缺页异常，其他单位还有分段，段页等。
	用户空间内存，从低到高分别是五种不同的内存段：
	只读段：包括代码和常量
	数据段：包括全局变量
	堆：包括动态分配的内存
	栈：包括局部变量和函数调用的上下文等
	文件映射段：包括动态库、共享内存等

	程序的源代码在经过编译后，会根据源代码的意义，分析出代码、数据等信息并存放在可执行文件上，如果不加调试信息的话，变量和函数的名称是不需要存储的，当计算机加载程序的时候会把代码、数据从可执行文件拷贝到不同的内存区域里面，同时，也会分配堆和栈的内存区域，但是在程序运行之前，堆和栈里面的内容是不确定的，也就是说变量的值此时是不确定的，堆和栈之间有着巨大的内存空白，这就让堆和栈有了充分的生长空间。

多核cpu	
	寄存器 - L1 Cache(SRAM) - L2 Cache(SRAM) - L3 Cache(SRAM) - 主存（DRAM）
	寄存器和L1 Cache：分为数据缓存和指令缓存，逻辑核独占
	L2 Cache：物理核独占，逻辑核共享
	L3 Cache、主存：所有物理核共享
	一般来说，每级缓存的命中率大概都在80%左右

	缓存一致性协议MESI
	缓存行：Cache line:缓存存储数据的单元，一般为64Byte
	MESI是指缓存行四种状态的首字母：
	M（Modified被修改） E（Exclusive 独享的）S(Shared共享的) I(Invalid 无效的)
	MESI协议解决了缓存数据可见性的问题，MESI能严格的保证缓存的一致性，但它存在性能问题，想修改一个变量，需要通知所有其他cpu并收到所有的确认回复，更槽的是当收到通知时其他cpu并不一定都处于空闲状态，可能无法在当前指令周期完成数据的状态变更，因此即使MESI协议能很好的保证缓存数据的一致性，但现代处理器多数采用的仍是MESI协议的变种，弱一致性的协议。后续又引入了store buffer和invalidate queue来提升cpu性能，但这会导致缓存数据的强一致性又被打破，只能是保证最终一致性。而某些数据的可见性也出现了延迟，这在代码层面上就会表现出某些变量后修改的却比另一些先修改的变量要先可见，给人一种代码执行顺序错乱的感觉，这种因为内存原因导致的指令乱序也是cpu指令重排序的一种。如果这时必须保证缓存行的强一致性即实时可见性的话就需要再引入另一个关键的指令：内存屏障

操作系统

	线程与进程
		进程是操作系统资源分配的基本单位，线程是操作系统进行调度的基本单位。
		进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈。
		进程的创建是通过fork，而创建线程是通过clone。
		线程的上下文切换分两种情况：
		1.不同进程的线程，切换的过程和进程上下文切换一样
		2.两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据

		进程间通信（IPC）
		管道 消息队列 共享内存 信号量和PV操作 信号 套接字

		同步与互斥
		进程/线程 同步：不同的进程/线程按照一定顺序来执行
		进程/线程 互斥：不同的进程/线程按照不能在同一个时刻一起执行，主要是为了保护临界资源

		常见的进程同步与互斥机制有两种：
		1.信号量与PV操作
		2.管程

	用户态和内核态
		当进程运行在内核空间时就处于内核态，进程运行在用户空间时则处于用户态
		在内核态下，进程运行在内核地址空间中，此时CPU可以执行任何指令，运行的代码也不受任何的限制
		在用户态下，进程运行在用户地址空间中，被执行的代码要受到CPU的诸多检查，它们只能访问映射其地址空间的页表项中规定的在用户态下可访问页面的虚拟地址
		区分内核态和用户态本质上是要提高操作系统的稳定性及可用性。

		如何从用户空间进入内核空间？
		1.系统调用 所有的系统资源管理都是在内核空间中完成的，比如读写磁盘文件，分配回收内存，从网络接口读写数据等等，我们的应用程序是无法直接进行这样的操作的，但是我们可以通过内核提供的接口来完成这样的任务。
		2.异常 比如缺页异常
		3.外围设备的中断

		对于一个进程来讲，从用户空间进入内核空间并最终返回到用户空间，这个过程是十分复杂的，举个例子，“堆栈”，其实进程在内核态和用户态各有一个堆栈，运行在用户空间时进程使用的是用户空间中的堆栈，而运行在内核空间时，进程使用的是内核空间中的堆栈，也就是说，Linux中每个进程有两个栈，分别位于用户态和内核态。

		用户态和内核态切换的开销大在哪里？
		1.保留用户态现场（上下文、寄存器、用户栈等）2.复制用户态参数，用户栈切换到内核栈，进入内核态 3.额外的检查 4.执行内核态代码 5.复制内核态代码执行结果，回到用户态 6.恢复用户态现场

		重点：
		用户态内核态的切换代价非常高，努力减少或避免状态切换的开销是很多程序员一直努力在做的事情，因为这部分开销也是我们程序性能最大的瓶颈了。
		可以简单的看下为了降低状态切换开销，我们都做过什么事情：
		1.cpu
		线程的创建、销毁需要进入内核态，多线程程序如果频繁创建线程将是非常恐怖的，所以我们使用线程池。
		线程的上下文切换也会进入内核态，所以线程不是越多越好，而为了减少线程的切换，我们用少量的线程承载用户自定义线程-协程。此外还有多路IO复用一个线程的技术-IO多路复用。
		线程中锁是很重要的概念，重量级锁需要借助内核的互斥量mutex来加锁，而真正的锁竞争又不是很频繁，所以我们经常使用自旋锁来避免进入内核态，但自旋锁也有缺点，所以java中的synchronized关键字有着非常复杂的设计
		2.IO
		为了减少IO过程中，数据的内核-用户空间的多次拷贝，在ngix等服务器中常会使用零拷贝技术
		为了更快的进行文件读取，通过mmap使用户空间、内核空间的一块内存都映射到相同的物理页帧，这种方式减少了拷贝，非常适合大文件的拷贝。（mmap senfile都是实现零拷贝的方式）
		3.内存
		内存的申请最小是页，即使我们sbrk申请了1byte的内存，操作系统给我们的也是1整页，降低了系统的调用次数，内存的申请与归还都是惰性的。

并发编程
	并发相关术语：
	原子操作：一个函数（原语）或动作的指令序列不可分割，要么作为一个整体执行（不可中断），要么都不执行
	临界资源：一次仅允许一个线程独占使用的不可剥夺的资源
	临界区：线程访问临界资源的那段程序代码，一次仅允许一个线程在临界区执行
	互斥：当一个线程正在临界区中访问临界资源时，其他线程不能进入临界区
	同步：合作的并发线程需要按先后次序执行，例如：一个线程的执行依赖于合作线程的消息或者信号，当一个线程没有得到来自于合作线程的消息或者信号时需要阻塞等待，直到消息或者信号到达后才被唤醒
	死锁：多个线程全部阻塞，形成等待资源的循环链
	饥饿：一个就绪线程被调度程序长期忽视，不被调度执行，一个线程长期得不到资源

	并发编程中常见的三个问题：原子性 可见性和有序性
	原子性：一个或者多个操作在CPU执行的过程中不被中断的特性
	原子性和可见性的区别？
	原子性是指多个cpu指令不被中断，需要靠操作系统锁机制来实现，可见性是指某个cpu对变量的修改对其他cpu可见，避免出现脏读，需要靠MESI协议和内存屏障来实现，两者没有因果关系，是相互独立的，单核cpu中仍存在原子性问题，但是不存在可见性问题，原子性问题的源头是并发线程切换，可见性的源头是cpu多核高速缓存私有。

	互斥比原子性更为严格，原子性只是保证指令不被中断，互斥保证了一批指令在同一时刻只有一个线程在执行，所以互斥肯定保证了原子性，但原子性没有实现互斥，互斥必须使用锁来实现

	锁机制是多线程编程中最常用的同步机制，用来对多线程间共享的临界区进行保护，linux Pthreads提供了多种锁机制，其中互斥量（Mutex）和自旋锁（Spin Lock）是内核中两个基础的锁，其他锁都是基于这两个锁来实现的。

	锁的本质是什么？锁的本质就是一个状态位，加解锁就是对这个状态位的修改，对临界资源上锁本质就是在进入临界区时加入对该状态位的判断。

	并发编程的关键是对于线程调度的理解，在一些常见的错误案例中，往往都是未对临界资源进行保护，对临界资源的访问到中途某一句代码即被终止，此时另外一个线程在CPU上进行运行态再次修改了临界资源，当前一个资源恢复为运行态时，此时的临界资源已经发生了变化，而执行还是在断点处恢复重新执行，因此发生了错误。

	自旋锁(spin lock)和互斥量(mutex)的比较
	自旋锁：当某个线程需要获取自旋锁，但该锁已经被其他线程占用时，该线程不会被挂起，而是在不断的消耗cpu的时间，不停自旋试图获取自旋锁，优点是没有昂贵的系统调用，一直处于用户态，执行速度快，缺点是会一直占用cpu，具体的实现比如CAS。
	互斥量：互斥量是阻塞锁，当某个线程无法获取互斥量时，该线程会被直接挂起，不再消耗CPU时间，当其他线程释放互斥量后，操作系统会激活那个被挂起的线程，让其投入运行。优点是不会忙等，得不到锁会sleep，缺点是sleep时会陷入到内核态，需要昂贵的系统调用。
	两种锁适用于不同的场景：
	如果是多核处理器，且预计线程等待锁的时间很短，短到比线程两次上下文切换时间要少的情况下，使用自旋锁是划算的。
	如果是多核处理器，且预计线程等待锁的时间较长，至少比两次线程上下文切换的时间要长，建议使用互斥量。
	如果是单核处理器，一般建议不要使用自旋锁，因为，在同一个时间只有一个线程是处在运行状态，那如果运行线程发现无法获取锁，只能等待解锁，但因为自身不挂起，所以那个获取到锁的线程没有办法进行运行状态，只能等到运行线程把操作系统分配给它的时间片用完，才能有机会被调度，这种情况下使用自旋锁的代价很高。
	如果加锁的代码经常被调用，但竞争情况很少发生时，应该优先考虑使用自旋锁。

	两种锁的本质区别：自旋锁获取失败的线程不会直接挂起，而是一直自旋占用cpu时间，直到自己的时间片使用完，然后等待下一次调度，相当于就算获取锁失败了，线程依然在正常执行，只不过一直在空跑。
	而互斥锁获取失败的线程直接挂起进入睡眠，立刻让出cpu，并且在唤醒之前不会再被cpu调度，使用mutex，获取锁失败的线程挂起唤醒需要切换内核态，因此成本比较高，而自旋锁则没有这种成本。

	总结
	单核cpu需要内存屏障吗？ 不需要

	单核cpu会有线程安全问题吗？ 有

	单核cpu有必要使用自旋锁吗？ 没必要

Linux
	文件
		Linux 支持很多文件类型，其中非常重要的文件类型有: 普通文件，目录文件，链接文件，设备文件，管道文件，Socket 套接字文件 等。
		普通文件（-）：用于存储信息和数据， Linux 用户可以根据访问权限对普通文件进行查看、更改和删除。比如：图片、声音、PDF、text、视频、源代码等等。
		目录文件（d，directory file）：目录也是文件的一种，用于表示和管理系统中的文件，目录文件中包含一些文件名和子目录名。打开目录事实上就是打开目录文件。
		符号链接文件（l，symbolic link）：保留了指向文件的地址而不是文件本身。
		字符设备（c，char）：用来访问字符设备比如键盘。
		设备文件（b，block）：用来访问块设备比如硬盘、软盘。
		管道文件(p，pipe) : 一种特殊类型的文件，用于进程之间的通信。
		套接字文件(s，socket)：用于进程间的网络通信，也可以用于本机之间的非网络通信。

	常见目录说明
		/bin： 存放二进制可执行文件(ls、cat、mkdir 等)，常用命令一般都在这里；	
		/etc： 存放系统管理和配置文件；
		/home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户 user 的主目录就是/home/user，可以用~user 表示；
		/dev： 用于存放设备文件；
		/var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等；

	常用命令
		cd ls ll mkdir find pwd rm cp mv touch cat more less tail vim tar scp chmod su(Switch User) 
		top vmstat free df du ps systemctl ping ifconfig netstat ss sudo grep kill

	环境变量修改
		通过 export命令可以修改指定的环境变量。不过，这种方式修改环境变量仅仅对当前 shell 终端生效，关闭 shell 终端就会失效。修改完成之后，立即生效。
		export CLASSPATH=./JAVA_HOME/lib;$JAVA_HOME/jre/lib
		通过 vim 命令修改环境变量配置文件。这种方式修改环境变量永久有效。
		vim ~/.bash_profile
		如果修改的是系统级别环境变量则对所有用户生效，如果修改的是用户级别环境变量则仅对当前用户生效。修改完成之后，需要 source 命令让其生效或者关闭 shell 终端重新登录。
		source /etc/profile

网络
	TCP
		TCP/IP四层模型
			TCP/IP 四层模型是目前被广泛采用的一种模型,我们可以将TCP/IP模型看作是OSI七层模型的精简版本，由以下 4 层组成：
			1.应用层 
			定义了信息交换的格式，消息会交给下一层传输层来传输。我们把应用层交互的数据单元称为报文。应用层协议定义了网络通信规则，对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如HTTP、SMTP、FTP、SSH、DNS
			2.传输层 
			传输层的主要任务就是负责向两台终端设备进程之间的通信提供通用的数据传输服务。传输层常见协议：
			TCP（Transmisson Control Protocol，传输控制协议）：提供面向连接的，可靠的数据传输服务。
			UDP（User Datagram Protocol，用户数据协议）：提供 无连接 的，尽最大努力 的数据传输服务（不保证数据传输的可靠性），简单高效。
			3.网络层 
			网络层负责为分组交换网上的不同主机提供通信服务。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报，简称数据报。网络层的还有一个任务就是选择合适的路由，使源主机运输层所传下来的分组，能通过网络层中的路由器找到目的主机。
			网络层常见协议：
			IP（Internet Protocol，网际协议）：TCP/IP 协议中最重要的协议之一，主要作用是定义数据包的格式、对数据包进行路由和寻址，以便它们可以跨网络传播并到达正确的目的地。
			ARP（Address Resolution Protocol，地址解析协议）：ARP 协议解决的是网络层地址和链路层地址之间的转换问题。因为一个 IP 数据报在物理上传输的过程中，总是需要知道下一跳（物理上的下一个目的地）该去往何处，但 IP 地址属于逻辑地址，而 MAC 地址才是物理地址，ARP 协议解决了 IP 地址转 MAC 地址的一些问题。
			NAT（Network Address Translation，网络地址转换协议）：NAT 协议的应用场景如同它的名称——网络地址转换，应用于内部网到外部网的地址转换过程中。具体地说，在一个小的子网（局域网，LAN）内，各主机使用的是同一个 LAN 下的 IP 地址，但在该 LAN 以外，在广域网（WAN）中，需要一个统一的 IP 地址来标识该 LAN 在整个 Internet 上的位置。
			4.网络接口层
			我们可以把网络接口层看作是数据链路层和物理层的合体。
			数据链路层(data link layer)通常简称为链路层（ 两台主机之间的数据传输，总是在一段一段的链路上传送的）。数据链路层的作用是将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。
			物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异

			四层网络模型每层各司其职，消息在进入每一层时都会多加一个报头，每多一个报头可以理解为数据报多戴一顶帽子。这个报头上面记录着消息从哪来，到哪去，以及消息多长等信息。比如，mac头部记录的是硬件的唯一地址，IP头记录的是从哪来和到哪去，传输层头记录到是到达目的主机后具体去哪个进程。

		握手/挥手
			三次握手
			一次握手:客户端发送带有 SYN（seq=x） 标志的数据包 -> 服务端，然后客户端进入 SYN_SEND 状态，等待服务器的确认；
			二次握手:服务端发送带有 SYN+ACK(seq=y,ACK=x+1) 标志的数据包 –> 客户端,然后服务端进入 SYN_RECV 状态
			三次握手:客户端发送带有 ACK(ACK=y+1) 标志的数据包 –> 服务端，然后客户端和服务器端都进入ESTABLISHED 状态，完成 TCP 三次握手。
			SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。

			四次挥手
			第一次挥手：客户端发送一个 FIN（SEQ=x） 标志的数据包->服务端，用来关闭客户端到服务器的数据传送。然后客户端进入 FIN-WAIT-1 状态。
			第二次挥手：服务器收到这个数据包，它发送一个 ACK （ACK=x+1）标志的数据包->客户端 。然后服务端进入 CLOSE-WAIT 状态，客户端进入 FIN-WAIT-2 状态。
			第三次挥手：服务端发送一个 FIN (SEQ=y)标志的数据包->客户端，请求关闭连接，然后服务端进入 LAST-ACK 状态。
			第四次挥手：客户端发送 ACK (ACK=y+1)标志的数据包->服务端，然后客户端进入TIME-WAIT状态，服务端在收到 ACK (ACK=y+1)标志的数据包后进入 CLOSE 状态。此时如果客户端等待 2MSL 后依然没有收到回复，就证明服务端已正常关闭，随后客户端也可以关闭连接了。

			为什么要四次挥手？
			TCP 是全双工通信，可以双向传输数据。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了 TCP 连接。
			举个例子：A 和 B 打电话，通话即将结束后。
			第一次挥手：A 说“我没啥要说的了”
			第二次挥手：B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话
			第三次挥手：于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”
			第四次挥手：A 回答“知道了”，这样通话才算结束。

			为什么不能把服务器发送的 ACK 和 FIN 合并起来，变成三次挥手？
			因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复 ACK，表示接收到了断开连接的请求。等到数据发完之后再发 FIN，断开服务器到客户端的数据传送。

			为什么第四次挥手客户端需要等待 2*MSL（报文段最长寿命）时间后才进入 CLOSED 状态？
			第四次挥手时，客户端发送给服务器的 ACK 有可能丢失，如果服务端因为某些原因而没有收到 ACK 的话，服务端就会重发 FIN，如果客户端在 2*MSL 的时间内收到了 FIN，就会重新发送 ACK 并再次等待 2MSL，防止 Server 没有收到 ACK 而不断重发 FIN。


		可靠性
			1.基于数据块传输：应用数据被分割成 TCP 认为最适合发送的数据块，再传输给网络层，数据块被称为报文段或段。
			2.对失序数据包重新排序以及去重：TCP 为了保证不发生丢包，就给每个包一个序列号，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据就可以实现数据包去重。
			3.校验和 : TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。
			4.超时重传 : 当发送方发送数据之后，它启动一个定时器，等待目的端确认收到这个报文段。接收端实体对已成功收到的包发回一个相应的确认信息（ACK）。如果发送端实体在合理的往返时延（RTT）内未收到确认消息，那么对应的数据包就被假设为已丢失open in new window并进行重传。
			5.流量控制 : TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议（TCP 利用滑动窗口实现流量控制）。
			6.拥塞控制 : 当网络拥塞时，减少数据的发送。

			流量控制
			TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。
			为什么需要流量控制? 
			这是因为双方在通信的时候，发送方的速率与接收方的速率是不一定相等，如果发送方的发送速率太快，会导致接收方处理不过来。如果接收方处理不过来的话，就只能把处理不过来的数据存在 接收缓冲区(Receiving Buffers) 里（失序的数据包也会被存放在缓存区里）。如果缓存区满了发送方还在狂发数据的话，接收方只能把收到的数据包丢掉。出现丢包问题的同时又疯狂浪费着珍贵的网络资源。因此，我们需要控制发送方的发送速率，让接收方与发送方处于一种动态平衡才好。
			1.发送方接收到了对方发来的报文 ack = 33, win = 10，知道对方收到了 33 号前的数据，现在期望接收 [33, 43) 号数据。那我们开始发送[33, 43) 号的数据。
			2.[33, 43) 号的数据你是已经发送了,但接受方并没有接受到[36,37]数据。所以接收方发送回对报文段 A 的确认：ack = 35, win = 10。
			3.发送方收到了 ack = 35, win = 10，对方期望接收 [35, 45) 号数据。那么发送方在发送[35, 45) 。
			这里面需要思考一个问题？
			第一步发送了[33, 43),如果这次发送[35, 45),那中间重叠部分不是发送了两次,所以这里要思考: 是全部重新发送还是只发送接收端没有收到的数据,如果全部发送那么重复
			发送的数据接收端怎么处理。这个下面快速重传会讲。
			4.接收方接收到了报文段 [35, 41)，接收方发送：ack = 41, win = 10. （这是一个累积确认）
			5.发送方收到了 ack = 41, win = 10，对方期望接收 [41, 51) 号数据。
			6. .......

			拥塞控制
			一句话概括：只要网络没有出现拥塞，拥塞窗口就增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少
			注入到网络中的分组数。
			在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。
			
			TCP 的拥塞控制采用了以下策略来减少网络拥塞的发生。
			慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。
			拥塞避免：慢启动中拥塞窗口的cwnd值,开始是1,接下开是指数型增涨的。1、2、4、8、16.....这样涨太快了吧。那么就有了堵塞避免。cwnd不能一直这样无限增长下去，一定需要某个限制。TCP使用了一个叫慢启动门限(ssthresh)的变量，一旦cwnd>=ssthresh（大多数TCP的实现，通常大小都是65536），慢启动过程结束，拥塞避免阶段开始；非ECN环境下的拥塞判断，发送方RTO超时，重传了一个报文段，它的逻辑如下：1.把ssthresh降低为cwnd值的一半。2.把cwnd重新设置为1。3.重新进入慢启动过程。
			3、快速重传
			TCP要保证所有的数据包都可以到达，所以，必需要有重传机制。
			注意: 接收端给发送端的Ack确认只会确认最后一个连续的包，比如，发送端发了1,2,3,4,5一共五份数据，接收端收到了1，2，于是回ack 3，然后收到了4（注意此时3没收到）此时的TCP会怎么办？我们要知道，因为正如前面所说的，SeqNum和Ack是以字节数为单位，所以ack的时候，不能跳着确认，只能确认最大的连续收到的包，不然，发送端就以为之前的都收到了。于是，TCP引入了一种叫Fast Retransmit的算法，不以时间驱动，而以数据驱动重传。也就是说，如果，包没有连续到达，就ack最后那个可能被丢了的包，如果发送方连续收到3次相同的ack，就重传。Fast Retransmit的好处是不用等timeout了再重传,而是只是三次相同的ack就重传。
			比如：如果发送方发出了1，2，3，4，5份数据，第一份先到送了，于是就ack回2，结果2因为某些原因没收到，3到达了，于是还是ack回2，后面的4和5都到了，但是还是ack回2因为2还是没有收到，于是发送端收到了三个ack=2的确认，知道了2还没有到，于是就马上重转2。然后，接收端收到了2，此时因为3，4，5都收到了，于是ack回6。
			https://zhuanlan.zhihu.com/p/471415273

			粘包
			TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议。其中跟粘包关系最大的就是基于字节流这个特点。字节流可以理解为一个双向的通道里流淌的数据，这个数据其实就是我们常说的二进制数据，简单来说就是一大堆 01 串。这些 01 串之间没有任何边界。
			应用层传到 TCP 协议的数据，不是以消息报为单位向目的主机发送，而是以字节流的方式发送到下游，这些数据可能被切割和组装成各种数据包，接收端收到这些数据包后没有正确还原原来的消息，因此出现粘包现象。
			粘包出现的根本原因是不确定消息的边界。接收端在面对"无边无际"的二进制流的时候，根本不知道收了多少 01 才算一个消息。一不小心拿多了就说是粘包。其实粘包根本不是 TCP 的问题，是使用者对于 TCP 的理解有误导致的一个问题。
			只要在发送端每次发送消息的时候给消息带上识别消息边界的信息，接收端就可以根据这些信息识别出消息的边界，从而区分出每个消息。
			常见的方法有：
			加入特殊标志，可以通过特殊的标志作为头尾，比如当收到了0xfffffe或者回车符，则认为收到了新消息的头，此时继续取数据，直到收到下一个头标志0xfffffe或者尾部标记，才认为是一个完整消息。
			加入消息长度信息，这个一般配合上面的特殊标志一起使用，在收到头标志时，里面还可以带上消息长度，以此表明在这之后多少 byte 都是属于这个消息的。如果在这之后正好有符合长度的 byte，则取走，作为一个完整消息给应用层使用。在实际场景中，HTTP 中的Content-Length就起了类似的作用，当接收端收到的消息长度小于 Content-Length 时，说明还有些消息没收到。那接收端会一直等，直到拿够了消息或超时。

			跟 TCP 同为传输层的另一个协议，UDP，User Datagram Protocol。用户数据包协议，是面向无连接，不可靠的，基于《数据报》的传输层通信协议。
			基于数据报是指无论应用层交给 UDP 多长的报文，UDP 都照样发送，即一次发送一个报文。至于如果数据包太长，需要分片，那也是IP层的事情，大不了效率低一些。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。而接收方在接收数据报的时候，也不会像面对 TCP 无穷无尽的二进制流那样不清楚啥时候能结束。正因为基于数据报和基于字节流的差异，TCP 发送端发 10 次字节流数据，而这时候接收端可以分 100 次去取数据，每次取数据的长度可以根据处理能力作调整；但 UDP 发送端发了 10 次数据报，那接收端就要在 10 次收完，且发了多少，就取多少，确保每次都是一个完整的数据报。

			https://mp.weixin.qq.com/s/0-YBxU1cSbDdzcZEZjmQYA

	HTTP
		HTTP 1.0 vs HTTP 1.1
		为了解决 HTTP/1.0 存在的资源浪费的问题， HTTP/1.1 优化为默认长连接模式。如果TCP连接一直保持的话也是对资源的浪费，因此，一些服务器软件（如 Apache）还会支持超时时间的时间。在超时时间之内没有新的请求达到，TCP 连接才会被关闭。有必要说明的是，HTTP/1.0 仍提供了长连接选项，即在请求头中加入Connection: Keep-alive。同样的，在 HTTP/1.1 中，如果不希望使用长连接选项，也可以在请求头中加入Connection: close。HTTP/1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。

		常见响应码
		200 OK 
		302 Found：资源被临时重定向了。比如你的网站的某些资源被暂时转移到另外一个网址。
		400 Bad Request：发送的 HTTP 请求存在问题。比如请求参数不合法、请求方法错误。
		401 Unauthorized：未认证却请求需要认证之后才能访问的资源。
		403 Forbidden：直接拒绝 HTTP 请求，不处理。一般用来针对非法请求。
		404 Not Found：你请求的资源未在服务端找到。比如你请求某个用户的信息，服务端并没有找到指定的用户。
		500 Internal Server Error：服务端出问题了（通常是服务端出 Bug 了）。比如你服务端处理请求的时候突然抛出异常，但是异常并未在服务端被正确处理。
		502 Bad Gateway：我们的网关将请求转发到服务端，但是服务端返回的却是一个错误的响应。

		HTTP vs HTTPS
		HTTPS 协议（Hyper Text Transfer Protocol Secure），是 HTTP 的加强安全版本。HTTPS 是基于 HTTP 的，也是用 TCP 作为底层协议，并额外使用 SSL/TLS 协议用作加密和安全认证。默认端口号是 443
		SSL/TLS 的核心要素是非对称加密。非对称加密采用两个密钥——一个公钥，一个私钥。在通信时，私钥仅由解密者保存，公钥由任何一个想与解密者通信的发送者（加密者）所知。
		为了公钥传输的信赖性问题，第三方机构应运而生——证书颁发机构（CA，Certificate Authority）。CA 默认是受信任的第三方。CA 会给各个服务器颁发证书，证书存储在服务器上，并附有 CA 的电子签名。
		当客户端（浏览器）向服务器发送 HTTPS 请求时，一定要先获取目标服务器的证书，并根据证书上的信息，检验证书的合法性。一旦客户端检测到证书非法，就会发生错误。客户端获取了服务器的证书后，由于证书的信任性是由第三方信赖机构认证的，而证书上又包含着服务器的公钥信息，客户端就可以放心的信任证书上的公钥就是目标服务器的公钥。

	DDoS攻击
	DDos 全名 Distributed Denial of Service，翻译成中文就是分布式拒绝服务。指的是处于不同位置的多个攻击者同时向一个或数个目标发动攻击，是一种分布的、协同的大规模攻击方式。单一的 DoS 攻击一般是采用一对一方式的，它利用网络协议和操作系统的一些缺陷，采用欺骗和伪装的策略来进行网络攻击，使网站服务器充斥大量要求回复的信息，消耗网络带宽或系统资源，导致网络或系统不胜负荷以至于瘫痪而停止提供正常的网络服务。
	举个例子
	我开了一家有五十个座位的重庆火锅店，由于用料上等，童叟无欺。平时门庭若市，生意特别红火，而对面二狗家的火锅店却无人问津。二狗为了对付我，想了一个办法，叫了五十个人来我的火锅店坐着却不点菜，让别的客人无法吃饭。
	上面这个例子讲的就是典型的 DDoS 攻击，一般来说是指攻击者利用“肉鸡”对目标网站在较短的时间内发起大量请求，大规模消耗目标网站的主机资源，让它无法正常服务。在线游戏、互联网金融等领域是 DDoS 攻击的高发行业。
	SYN Flood 是互联网上最原始、最经典的 DDoS 攻击之一，旨在耗尽可用服务器资源，致使服务器无法传输合法流量。
	SYN Flood 利用了 TCP 协议的三次握手机制，攻击者通常利用工具或者控制僵尸主机向服务器发送海量的变源 IP 地址或变源端口的 TCP SYN 报文，服务器响应了这些报文后就会生成大量的半连接，当系统资源被耗尽后，服务器将无法提供正常的服务。 增加服务器性能，提供更多的连接能力对于 SYN Flood 的海量报文来说杯水车薪，防御SYN Flood的关键在于判断哪些连接请求来自于真实源，屏蔽非真实源的请求以保障正常的业务请求能得到服务。

IO	
	输入设备（比如键盘）和输出设备（比如显示器）都属于外部设备。网卡、硬盘这种既可以属于输入设备，也可以属于输出设备。输入设备向计算机输入数据，输出设备接收计算机输出的数据。从计算机结构的视角来看的话， I/O 描述了计算机系统与外部设备之间通信的过程。
	为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 用户空间（User space） 和 内核空间（Kernel space ）。像我们平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源有关的操作，比如文件管理、进程通信、内存管理等等。也就是说，我们想要进行 IO 操作，一定是要依赖内核空间的能力。并且，用户空间的程序不能直接访问内核空间。当想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成。因此，用户进程想要执行 IO 操作的话，必须通过 系统调用 来间接访问内核空间。
	我们在平常开发过程中接触最多的就是 磁盘IO（读写文件）和 网络IO（网络请求和响应）。从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。当应用程序发起 I/O 调用后，会经历两个步骤：
	1.内核等待 I/O 设备准备好数据
	2.内核将数据从内核空间拷贝到用户空间。

	系统调用
	系统调用是内核内置的一系列函数，绝大多数都可以在c语言中直接调用。例如打开文件的open函数，读文件read，io多路复用的epoll、poll、select，申请内存的brk/mmap，多线程相关的futex都是耳熟能详的经典系统调用。为了将知识点串联起来，我们看一下读文件过程中，程序、操作系统和硬件都干了什么？
	伪代码大概就是下面三行，打开，读取，关闭。
	fd = open filePath
	data = read fd
	do something with data
	close fd
	首先进程通过open这个系统调用，进入内核态，由内核打开文件，获得文件描述符，并将fd返回到用户空间，然后read系统调用，进入内核态，内核去读取这个文件中的内容，读文件的过程可以展开说下：
	1.通过文件名到磁盘inode区以文件全名作为key查到对应的inode信息，inode中记录了该文件实际存储的块（block）的编号，可能有多个。
	2.块信息拿到之后需要转换为扇区(sector)标号，因为块是逻辑存储单位，而扇区才是硬盘的存储单位，一个块由1个或者多个扇区组成。接下来就是读取磁盘了，也就是io过程，会交给南桥中的dma控制器来接手
	3.dma接手后控制硬盘的磁头到指定的扇区读取磁盘数据，并将读到的数据直接写到内存的内核空间
	4.等读磁盘完成之后，dma控制器会向cpu发送一个中断，来告知cpu读取完成以及读取到的数据所放的内存地址。
	5.cpu处理中断，拿到内核空间中数据的地址，并将这部分内存数据拷贝到用户空间，使得用户程序中能够得到上面data这个变量，然后就从内核态退回到用户态了
	6.最后就是关闭文件描述符了。

	IO模型 （参考ata《从java BIO到NIO再到多路复用，看这篇就够了》）
		BIO (Blocking I/O)
		同步阻塞 IO 模型 
		在传统的 Java I/O 模型（BIO）中，I/O 操作是以阻塞的方式进行的。也就是说，当一个线程执行一个 I/O 操作时，它会被阻塞直到操作完成。这种阻塞模型在处理多个并发连接时可能会导致性能瓶颈，因为需要为每个连接创建一个线程，而线程的创建和切换都是有开销的。
		BIO属于同步阻塞IO模型。同步阻塞IO模型中，应用程序发起read调用后，会一直阻塞，直到内核把数据拷贝到用户空间。

		NIO (Non-blocking/New I/O)
		Java 中的 NIO 于 Java 1.4 中引入，对应 java.nio 包，提供了 Channel，Selector，Buffer等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它是支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。
		同步非阻塞 IO 模型中，应用程序会一直发起read调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。这个时候，I/O 多路复用模型 就上场了。需要注意：使用NIO并不一定意味着高性能，它的性能优势主要体现在高并发和高延迟的网络环境下。当连接数较少、并发程度较低或者网络传输速度较快时，NIO 的性能并不一定优于传统的 BIO 。

		IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -> 用户空间）还是阻塞的。

		目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。
		select 调用：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。epoll 调用：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。

		Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。

		Channel（通道）
		Channel 是一个通道，它建立了与数据源（如文件、网络套接字等）之间的连接。我们可以利用它来读取和写入数据，就像打开了一条自来水管，让数据在 Channel 中自由流动。其中，最常用的是以下几种类型的通道：
		FileChannel：文件访问通道；
		SocketChannel、ServerSocketChannel：TCP通信通道；
		DatagramChannel：UDP 通信通道；
		Channel 最核心的两个方法：
		read ：读取数据并写入到 Buffer 中。
		write ：将 Buffer 中的数据写入到 Channel 中。

		Selector（选择器）
		Selector（选择器） 是 NIO中的一个关键组件，它允许一个线程处理多个 Channel。Selector 是基于事件驱动的 I/O 多路复用模型，主要运作原理是：通过 Selector 注册通道的事件，Selector 会不断地轮询注册在其上的 Channel。当事件发生时，比如：某个 Channel上面有新的 TCP 连接接入、读和写事件，这个 Channel就处于就绪状态，会被 Selector 轮询出来。Selector 会将相关的 Channel加入到就绪集合中。通过 SelectionKey可以获取就绪 Channel的集合，然后对这些就绪的 Channel进行响应的 I/O 操作。

		NIO 零拷贝
		零拷贝是提升 IO 操作性能的一个常用手段，像 ActiveMQ、Kafka、RocketMQ、QMQ、Netty等顶级开源项目都用到了零拷贝。零拷贝是指计算机执行 IO 操作时，CPU 不需要将数据从一个存储区域复制到另一个存储区域，从而可以减少上下文切换以及 CPU 的拷贝时间。也就是说，零拷贝主主要解决操作系统在处理 I/O 操作时频繁复制数据的问题。零拷贝的常见实现技术有： mmap+write、sendfile和 sendfile + DMA gather copy 。零拷贝主要是减少了 CPU 拷贝及上下文的切换。
		Java 对零拷贝的支持：
		MappedByteBuffer 是 NIO 基于内存映射（mmap）这种零拷贝⽅式的提供的⼀种实现，底层实际是调用了 Linux 内核的 mmap 系统调用。它可以将一个文件或者文件的一部分映射到内存中，形成一个虚拟内存文件，这样就可以直接操作内存中的数据，而不需要通过系统调用来读写文件。
		FileChannel 的transferTo()/transferFrom()是 NIO 基于发送文件（sendfile）这种零拷贝方式的提供的一种实现，底层实际是调用了 Linux 内核的 sendfile系统调用。它可以直接将文件数据从磁盘发送到网络，而不需要经过用户空间的缓冲区。

		AIO (Asynchronous I/O)
		AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。
		异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。
		一个多路复用器 Selector 可以同时轮询多个 Channel，由于 JDK使用了 epoll() 代替传统的 select 实现，所以它并没有最大连接句柄 1024/2048 的限制。这也就意味着只需要一个线程负责 Selector 的轮询，就可以接入成千上万的客户端。
		
		IO基本原理
		系统内核调用read、write操作负责将数据在用户缓冲区和内核缓冲区之间进行复制，系统内核负责内核缓冲区和物理设备（磁盘、网卡）之间的数据交换，内核缓冲区会经历数据准备、数据复制两个阶段。缓冲区的存在减少了与设备之间的频繁物理交换，外部设备的读写依赖OS中断，会引起上下文切换，性能开销较大。操作系统会对内核缓冲区进行监控，等缓冲区达到一定的数量，在进行IO设备的中断处理，集中执行物理设备的实际IO操作（数据准备），从而提升下性能。
		内核、网卡、用户空间，三者的关系是，内核跟网卡交互做数据准备，内核跟用户空间交互做数据复制，IO的核心是两个缓冲区的数据准备和数据复制的交互。

		c10k问题
		c10k（concurrent 10000 connections）简单来讲就是服务器如何处理10k个连接。
		
		BIO实现
		public class BIOServer {
		    public static void main(String[] args) throws IOException {
		        ServerSocket server = new ServerSocket(9998, 20);
		        System.out.println("server begin");
		        while(true){
		            //阻塞1
		            Socket client = server.accept();
		            new Thread(() -> {
		                InputStream in;
		                try{
		                    in = client.getInputStream();
		                    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
		                    while (true){
		                        //阻塞2
		                        String data = reader.readLine();
		                        if(null != data){
		                            System.out.println(data);
		                        } else {
		                            client.close();
		                            break;
		                        }
		                    }
		                    System.out.println("client break");
		                }catch (IOException e){
		                    e.printStackTrace();
		                }
		            }).start();
		        }
		    }
		}

		NIO实现
		public class NIOServer {
		    public static void main(String[] args) throws Exception{
		        LinkedList<SocketChannel> clients = new LinkedList<>();
		        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
		        serverSocketChannel.bind(new InetSocketAddress(9998));
		        //设置非阻塞
		        serverSocketChannel.configureBlocking(false);
		        while(true){
		            //设置非阻塞，线程不挂起，继续执行
		            SocketChannel client = serverSocketChannel.accept();
		            if(client == null){

		            } else {
		                client.configureBlocking(false);
		                clients.add(client);
		            }
		            ByteBuffer buffer = ByteBuffer.allocateDirect(4096);
		            for(SocketChannel c : clients){
		                //不阻塞
		                int num = c.read(buffer);
		                if(num > 0){
		                    buffer.flip();
		                    byte[] aaa = new byte[buffer.limit()];
		                    buffer.get(aaa);
		                    String b = new String(aaa);
		                    System.out.println(c.socket().getPort()+":"+b);
		                    buffer.clear();
		                }
		            }
		        }
		    }
		}
		NIO相比BIO的优势：建立连接和读写数据不阻塞，因为读数据不阻塞所以不用像BIO那样需要一个线程守着一个连接，可以直接用一个线程循环处理所有连接。但这样不好的地方就在于，只要有一个连接有数据进来，就需要遍历所有的客户端连接去读取数据，10k个客户端就得10k次系统调用，产生10k次的用户态和内核态来回切换，多路复用就是解决这个问题的。
		多路复用就是指一次系统调用，能得到多个客户端是否有读写事件，多路（多个客户端连接）复用（复用一次系统调用）。
		多路复用是依赖操作系统内核的能力，在linux中，多路复用分为两个阶段：
		阶段一：select&poll
		这两个指令实际就干了一件事，以前由用户态去循环遍历所有客户端产生系统调用，改成了由内核自己去遍历，如果select模式，只需10次系统调用（因为select最大支持1024个文件描述符查询，总共10k个连接），如果是poll模式，则没有数据的限制，只需要一次	
		阶段二：epoll
		除了select和poll的功能，epoll还实现了零拷贝技术

		select，poll，epoll的区别
		select，poll，epoll都是IO多路复用机制，即可以监视多个描述符，一旦某个描述符就绪（读或写就绪），能够通知程序进行相应读写操作。 但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

		select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。
		select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。

					select															poll				epoll
		性能		    随着连接数的增加，性能急剧下降，处理成千上万的并发连接数时，性能很差	同上					随着连接数的增加，性能基本没有变化
		连接数	    一般1024															无限制				无限制
		内存拷贝 	每次调用select拷贝												每次调用poll拷贝		fd首次调用epoll_ctl拷贝，每次调用epoll_wait不拷贝
		数据结构	    bitmap															数组					红黑树
		内在处理机制	线性轮询															线性轮询				FD挂在红黑树，通过事件回调callback
		时间复杂度	O(n)															O(n)				O(1)


	Netty
	上述的NIO多路复用有没有问题？
	如果此时来了一万个连接，都有读写请求，那么在一万个请求处理结束之前，它都没法接受新的请求，哪怕是最简单的链接请求，因为此时的处理线程只有一个。Netty就是为了解决这个问题。
	Netty是一款异步的事件驱动的网络应用程序框架，特性：高性能、高吞吐、低延迟、低资源消耗、最少的内存复制（零拷贝）,易用性、相比于Java提供的底层网络api
	NIO与Netty
	NIO的特性是同步非阻塞，Netty其实就是对NIO的一个包装和优化，在NIO的基础上，提供了完全异步的特性，并额外提供了上述相关特性。

	Reactor
	Reactor是利用NIO对IO线程进行不同的分工：
	使用前边我们提到的IO多路复用模型比如select,poll,epoll,kqueue,进行 IO 事件的注册和监听。
	将监听到就绪的IO事件分发dispatch 到各个具体的处理Handler中进行相应的IO事件处理。
	通过IO多路复用技术就可以不断的监听IO事件，不断的分发dispatch，就像一个反应堆一样，看起来像不断的产生IO事件，因此我们称这种模式为Reactor模型。
	下面我们来看下Reactor模型的三种分类：
	单 Reactor 单线程
	单Reactor意味着只有一个epoll对象，用来监听所有的事件，比如连接事件，读写事件。单线程意味着只有一个线程来执行epoll_wait获取IO就绪的Socket，然后对这些就绪的Socket执行读写，以及后边的业务处理也依然是这个线程。
	单Reactor单线程模型就好比我们开了一个很小很小的小饭馆，作为老板的我们需要一个人干所有的事情，包括：迎接顾客（accept事件），为顾客介绍菜单等待顾客点菜(IO请求)，做菜（业务处理），上菜（IO响应），送客（断开连接）。
	单 Reactor 多线程
	随着客人的增多（并发请求），显然饭馆里的事情只有我们一个人干（单线程）肯定是忙不过来的，这时候我们就需要多招聘一些员工（多线程）来帮着一起干上述的事情。于是就有了单Reactor多线程模型：这种模式下，也是只有一个epoll对象来监听所有的IO事件，一个线程来调用epoll_wait获取IO就绪的Socket。但是当IO就绪事件产生时，这些IO事件对应处理的业务Handler，我们是通过线程池来执行。这样相比单Reactor单线程模型提高了执行效率，充分发挥了多核 CPU 的优势。
	主从 Reactor 多线程
	做任何事情都要区分事情的优先级，我们应该优先高效的去做优先级更高的事情，而不是一股脑不分优先级的全部去做。当我们的小饭馆客人越来越多（并发量越来越大），我们就需要扩大饭店的规模，在这个过程中我们发现，迎接客人是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。
	于是，主从Reactor多线程模型就产生了：
	我们由原来的单Reactor变为了多Reactor。主Reactor用来优先专门做优先级最高的事情，也就是迎接客人（处理连接事件），对应的处理Handler就是图中的acceptor。
	当创建好连接，建立好对应的socket后，在acceptor中将要监听的read事件注册到从Reactor中，由从Reactor来监听socket上的读写事件。
	最终将读写的业务逻辑处理交给线程池处理。
	主从Reactor多线程模型是现在大部分主流网络框架中采用的一种IO线程模型。Netty就是用的这种模型。
	Reactor在netty中是以group的形式出现的，netty中将Reactor分为两组，一组是MainReactorGroup也就是我们在编码中常常看到的EventLoopGroup bossGroup,另一组是SubReactorGroup也就是我们在编码中常常看到的EventLoopGroup workerGroup。
	MainReactorGroup中通常只有一个Reactor，专门负责做最重要的事情，也就是监听连接accept事件。当有连接事件产生时，在对应的处理handler acceptor中创建初始化相应的NioSocketChannel（代表一个Socket连接）。然后以负载均衡的方式在SubReactorGroup中选取一个Reactor，注册上去，监听Read事件。
	MainReactorGroup中只有一个Reactor的原因是，通常我们服务端程序只会绑定监听一个端口，如果要绑定监听多个端口，就会配置多个Reactor。
	SubReactorGroup中有多个Reactor，具体Reactor的个数可以由系统参数 -D io.netty.eventLoopThreads指定。默认的Reactor的个数为CPU核数 * 2。SubReactorGroup中的Reactor主要负责监听读写事件，每一个Reactor负责监听一组socket连接。将全量的连接分摊在多个Reactor中。
	一个Reactor分配一个IO线程，这个IO线程负责从Reactor中获取IO就绪事件，执行IO调用获取IO数据，执行PipeLine。
	当IO请求在业务线程中完成相应的业务逻辑处理后，在业务线程中利用持有的ChannelHandlerContext引用将响应数据在PipeLine中反向传播，最终写回给客户端。


